# LLM Cross-Examination with Bible

**Edukacinio turinio kÅ«rimas naudojant dirbtinÄ¯ intelektÄ…** / **Developing Educational Content Using Artificial Intelligence**

University coursework by Balys Å½alneraviÄius

## Overview

This project explores the reliability and adequacy of large language models (LLMs) in generating and evaluating educational questions based on the Lithuanian translation of the Holy Bible. The project implements a cross-examination system where three LLM models (gemini-2.5-flasg, mistral-medium and mistral-small) evaluate questions generated by each other, creating a comprehensive quality assessment framework.

## Project Goals

- Generate Bible-based educational questions using multiple LLM models
- Evaluate question quality across different LLM models
- Identify perfectly formulated questions (grade 5 from both evaluators)
- Analyze LLM consistency and reliability in educational content creation
- Demonstrate cross-validation methodology for AI-generated educational materials

## Project Structure

```
â”œâ”€â”€ source_text/                 # Bible source material (Lithuanian)
â”‚   â”œâ”€â”€ jono_evangelija/         # Gospel of John
â”‚   â”œâ”€â”€ luko_evangelija/         # Gospel of Luke
â”‚   â”œâ”€â”€ mato_evangelija/         # Gospel of Matthew
â”‚   â””â”€â”€ morkaus_evangelija/      # Gospel of Mark
â”‚
â”œâ”€â”€ results/                     # All project outputs
â”‚   â”œâ”€â”€ questions/               # Generated questions by model
â”‚   â”œâ”€â”€ evaluations/             # Cross-model evaluations
â”‚   â”œâ”€â”€ all_questions_with_grade_5/  # **Perfect questions (grade 5)**
â”‚   â””â”€â”€ evaluation_charts/       # Statistics visualizations
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ main.py                  # Main execution script
â”‚   â”œâ”€â”€ llm_generation.py        # Question generation logic
â”‚   â”œâ”€â”€ llm_evaluation.py        # Question evaluation logic
â”‚   â”œâ”€â”€ stats.py                 # Statistical analysis
â”‚   â”œâ”€â”€ filter_perfect_questions.py  # Perfect question extraction
â”‚   â”œâ”€â”€ parser.py                # Text parsing utilities
â”‚   â””â”€â”€ file_io.py               # File I/O operations
â”‚
â”œâ”€â”€ bin/                         # Documentation & diagrams
â”‚   â””â”€â”€ diagrams/                # System architecture diagrams
â”‚
â””â”€â”€ README.md                    # This file
```

## Quick Start

### Perfect Questions (Grade 5 Results)

All questions that received a perfect grade of 5 from both evaluators are available in:

ğŸ“ **[results/all_questions_with_grade_5/](results/all_questions_with_grade_5/)**

Breakdown:
- **Gemini 2.5-flash**: 1,219 grade-5 questions
- **Mistral Medium**: 1,169 grade-5 questions  
- **Mistral Small**: 877 grade-5 questions
- **Total**: 3,265 grade-5 questions

Each model's questions are organized by Gospel:
- Jn (Gospel of John)
- Lk (Gospel of Luke)
- Mk (Gospel of Mark)
- Mt (Gospel of Matthew)

## System Architecture

### Question Generation Pipeline
![Question Generation](bin/diagrams/klausimÅ³%20generavimo%20ir%20vertinimo%20su%20LLM%20diagrama.png)

### Cross-Model Evaluation Process
![Model Evaluation](bin/diagrams/kaip%20modeliai%20vertino%20vieni%20kitus.png)

## Key Files

| File | Purpose |
|------|---------|
| `src/llm_generation.py` | Generate Bible questions using LLMs |
| `src/llm_evaluation.py` | Evaluate questions with cross-model assessment |
| `src/filter_perfect_questions.py` | Extract questions with grade 5 from both evaluators |
| `src/stats.py` | Generate statistics and visualization charts |

## Documentation

The detailed academic paper explaining methodology, findings, and conclusions is available in:
ğŸ“„ **Å½alneraviÄiusBalysKD2026.pdf**

